{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import datasets.transforms as T\n",
    "torch.set_grad_enabled(False);\n",
    "from models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO classes\n",
    "CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "]\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sof-DETR transforms for input image\n",
    "sof_transform = T.Compose([\n",
    "    T.RandomResize([800], max_size=1333),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting and saving it\n",
    "def plot_results(pil_img, prob, boxes,name,save=False):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    if save == True:\n",
    "\n",
    "        plt.savefig(name+\"_o.png\",bbox_inches='tight',dpi=600)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=10,\n",
    "                bbox=dict(facecolor='red', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    if save==True:\n",
    "\n",
    "        plt.savefig((name+\"_im.png\"),bbox_inches='tight',dpi=600)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sof-DETR Model-building\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.ArgumentParser='Sof-DETR'\n",
    "        self.lr=1e-4\n",
    "        self.lr_backbone=1e-5\n",
    "        self.batch_size=3 \n",
    "        self.weight_decay=1e-4\n",
    "        self.epochs=500\n",
    "        self.lr_drop=300\n",
    "        self.clip_max_norm=0.1\n",
    "\n",
    "        # Model parameters\n",
    "        self.frozen_weights=''\n",
    "        # * Backbone\n",
    "        self.backbone='resnet50'\n",
    "        self.dilation=False\n",
    "        self.position_embedding='sine'\n",
    "\n",
    "        # * Transformer\n",
    "        self.enc_layers=6\n",
    "        self.dec_layers=6\n",
    "        self.dim_feedforward=2048\n",
    "        self.hidden_dim=256\n",
    "        self.dropout=0.1\n",
    "        self.nheads=8\n",
    "        self.num_queries=100\n",
    "        self.pre_norm=False\n",
    "\n",
    "        # * Segmentation\n",
    "        self.masks=False\n",
    "        self.return_intermediate=True\n",
    "        # Loss\n",
    "        self.aux_loss=False\n",
    "        # * Matcher\n",
    "        self.set_cost_class=1\n",
    "        self.set_cost_bbox=5\n",
    "        self.set_cost_giou=2\n",
    "        # * Loss coefficients\n",
    "        self.mask_loss_coef=1\n",
    "        self.dice_loss_coef=1\n",
    "        self.bbox_loss_coef=5\n",
    "        self.giou_loss_coef=2\n",
    "        self.eos_coef=0.1\n",
    "        # dataset parameters\n",
    "        self.dataset_file='coco'\n",
    "        self.coco_path=''\n",
    "        self.coco_panoptic_path=''\n",
    "        self.remove_difficult=False\n",
    "\n",
    "        self.output_dir=''\n",
    "        self.device='cuda'\n",
    "        self.seed=42\n",
    "        self.resume=''\n",
    "        self.start_epoch=0\n",
    "        self.eval=False\n",
    "        self.num_workers=2\n",
    "\n",
    "        # distributed training parameters\n",
    "        self.world_size=1\n",
    "        self.dist_url='env://'\n",
    "args=Args()\n",
    "\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "# model.to(args.device)\n",
    "sof_model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sof_detr_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4399878877d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load pre-trained Sof-DETR Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msof_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../trained/checkpoint0499.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msof_detr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msof_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msof_detr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sof_detr_model' is not defined"
     ]
    }
   ],
   "source": [
    "#load pre-trained Sof-DETR Model\n",
    "sof_model=torch.load('../trained/checkpoint0499.pth')\n",
    "sof_detr_model.load_state_dict(sof_model['model'])\n",
    "sof_detr_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_of_tensors_to_tensor(tuple_of_tensors):\n",
    "    return  torch.stack(list(tuple_of_tensors), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to your image\n",
    "\n",
    "# url = 'http://images.cocodataset.org/val2017/000000150224.jpg' # 1\n",
    "url = 'http://images.cocodataset.org/val2017/000000332351.jpg'#2\n",
    "im = Image.open(requests.get(url, stream=True).raw)\n",
    "img_path,image_name=os.path.split(url)\n",
    "save_name=image_name[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the inpit image and nomlaize it \n",
    "img_temp = sof_transform(im,None)\n",
    "img=tuple_of_tensors_to_tensor(img_temp[0]).unsqueeze(0)\n",
    "# propagate through the model\n",
    "detections = sof_detr_model(img)\n",
    "\n",
    "# keeping only predictions with 0.9+ confidence value\n",
    "prob = detections['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "keep = prob.max(-1).values > 0.9\n",
    "\n",
    "# scaling back the bounding boxes \n",
    "bboxes_scaled = rescale_bboxes(detections['pred_boxes'][0, keep], im.size)\n",
    "plot_results(im, prob[keep], bboxes_scaled,save_name,save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to your image\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000150224.jpg' # 1\n",
    "# url = 'http://images.cocodataset.org/val2017/000000332351.jpg'#2\n",
    "im = Image.open(requests.get(url, stream=True).raw)\n",
    "img_path,image_name=os.path.split(url)\n",
    "save_name=image_name[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the inpit image and nomlaize it \n",
    "img_temp = sof_transform(im,None)\n",
    "img=tuple_of_tensors_to_tensor(img_temp[0]).unsqueeze(0)\n",
    "# propagate through the model\n",
    "detections = sof_detr_model(img)\n",
    "\n",
    "# keeping only predictions with 0.9+ confidence value\n",
    "prob = detections['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "keep = prob.max(-1).values > 0.9\n",
    "\n",
    "# scaling back the bounding boxes \n",
    "bboxes_scaled = rescale_bboxes(detections['pred_boxes'][0, keep], im.size)\n",
    "plot_results(im, prob[keep], bboxes_scaled,save_name,save=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
